{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/proj/inductive-bias.shadow/abakalov.anaconda/envs/sae/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/proj/inductive-bias.shadow/abakalov.anaconda/envs/sae/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers   \n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b\",\n",
    "    use_auth_token=True,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"./offload\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "if tokenizer.pad_token is None:\n",
    "    if tokenizer.eos_token is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    else:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "from models.gemma_2 import Gemma\n",
    "\n",
    "hooked_model = Gemma(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "with open(\"cache/accuracies_sorted_gemma.json\", \"r\") as file:\n",
    "    accuracies_sorted = json.load(file)\n",
    "with open(\"cache/steering_directions_gemma_sae_full.json\", \"r\") as file:\n",
    "    steering_directions_json = json.load(file)\n",
    "steering_directions = {}\n",
    "for item in steering_directions_json:\n",
    "    steering_directions[eval(item)] = np.array(steering_directions_json[item])\n",
    "with open(\"cache/stds_gemma_sae_full.json\", \"r\") as file:\n",
    "    stds_json = json.load(file)\n",
    "stds = {}\n",
    "for item in stds_json:\n",
    "    stds[eval(item)] = stds_json[item]\n",
    "\n",
    "accuracies_by_layer = {layer: np.mean([p[0] for p in accuracies_sorted if p[1] == layer]) for layer in range(26)}\n",
    "accuracies_by_layer_sorted = sorted([[acc, l] for l, acc in accuracies_by_layer.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "alpha = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [02:20,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 10 0.30392156862745096 -1.8297889057327719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tasks.truthfulqa\n",
    "task = tasks.truthfulqa.TruthfulQA(\"testing\")\n",
    "tokenized_dataset = task.get_tokenized_dataset(tokenizer=tokenizer, batch_size=16, subset=False,\n",
    "                        random_seed=42, subset_len=100, max_length=1000)\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=16, shuffle=False)\n",
    "correct = 0\n",
    "sum_logit_diff = 0\n",
    "positions_to_steer = [(t[1], h, -1) for t in accuracies_by_layer_sorted[:k] for h in range(8)]\n",
    "vectors = [alpha * steering_directions[pos[0]][pos[1] * 256:(pos[1] + 1) * 256] * stds[pos[0]] for pos in positions_to_steer]\n",
    "hooked_model.set_steering_vectors(vectors, positions_to_steer)\n",
    "with torch.no_grad():\n",
    "    for iter, batch in tqdm(enumerate(dataloader)):\n",
    "        batch_tokens = batch[\"tokens\"].cuda()\n",
    "        \n",
    "        logits = hooked_model.model(batch_tokens).logits.cpu()\n",
    "        correct_logits = [tokenizer.convert_tokens_to_ids(chr(ord(\"A\") + label)) for label in batch[\"correct_label\"]]\n",
    "        incorrect_logits = [\n",
    "                [\n",
    "                    tokenizer.convert_tokens_to_ids(chr(ord(\"A\") + label)) for label in range(num_labels) if label != correct_label\n",
    "                ]\n",
    "                for correct_label, num_labels in zip(batch[\"correct_label\"], batch[\"num_labels\"])\n",
    "            ]\n",
    "        incorrect_logits_probs = [np.array([logits[batch_i, -1, incorrect_id] for incorrect_id in incorrect_logits[batch_i]]) for batch_i in range(len(incorrect_logits))]\n",
    "        max_incorrect_logit = [incorrect_logits[batch_i][np.argmax(incorrect_logits_probs[batch_i])] for batch_i in range(len(incorrect_logits))]\n",
    "        logit_diff = [logits[batch_i, -1, correct_logits[batch_i]] - logits[batch_i, -1, max_incorrect_logit[batch_i]] for batch_i in range(len(incorrect_logits))]\n",
    "        \n",
    "        correct += (np.array(logit_diff) > 0).sum()\n",
    "        sum_logit_diff += (np.array(logit_diff)).sum()\n",
    "print(k, alpha, correct / len(tokenized_dataset), sum_logit_diff / len(tokenized_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
